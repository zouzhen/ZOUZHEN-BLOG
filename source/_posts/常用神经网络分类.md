---
title: 常用神经网络分类
date: 2019-04-17 09:09:34
categories: 深度学习
tags: [神经网络]
---
### 概述
本文主要介绍了当前常用的神经网络，这些神经网络主要有哪些用途，以及各种神经网络的优点和局限性。

### 1 BP神经网络
BP (Back Propagation)神经网络是一种神经网络学习算法。其由输入层、中间层、输出层组成的阶层型神经网络，中间层可扩展为多层。相邻层之间各神经元进行全连接，而每层各神经元之间无连接，网络按有教师示教的方式进行学习，当一对学习模式提供给网络后，各神经元获得网络的输入响应产生连接权值（Weight）。然后按减小希望输出与实际输出误差的方向，从输出层经各中间层逐层修正各连接权，回到输入层。此过程反复交替进行，直至网络的全局误差趋向给定的极小值，即完成学习的过程。

    初始权值阈值的确定：所以权值及阈值的初始值应选为均匀分布的小数经验

值，约为（-2.4/F~2.4/F）之间，其中F 为所连单元的输入层节点数

#### 1.1 主要功能
（1）函数逼近：用输入向量和相应的输出向量训练一个网络以逼近一个函数。

（2）模式识别：用一个待定的输出向量将它与输入向量联系起来。

（3）分类：把输入向量所定义的合适方式进行分类。

（4）数据压缩：减少输出向量维数以便传输或存储。

#### 1.2 优点及其局限性
BP神经网络最主要的优点是具有极强的非线性映射能力。理论上，对于一个三层和三层以上的BP网络，只要隐层神经元数目足够多，该网络就能以任意精度逼近一个非线性函数。其次，BP神经网络具有对外界刺激和输入信息进行联想记忆的能力。这是因为它采用了分布并行的信息处理方式，对信息的提取必须采用联想的方式，才能将相关神经元全部调动起来。BP 神经网络通过预先存储信息和学习机制进行自适应训练，可以从不完整的信息和噪声干扰中恢复原始的完整信息。这种能力使其在图像复原、语言处理、模式识别等方面具有重要应用。再次，BP 神经网络对外界输入样本有很强的识别与分类能力。由于它具有强大的非线性处理能力，因此可以较好地进行非线性分类, 解决了神经网络发展史上的非线性分类难题。另外， BP 神经网络具有优化计算能力。BP神经网络本质上是一个非线性优化问题, 它可以在已知的约束条件下，寻找一组参数组合，使该组合确定的目标函数达到最小。不过，其优化计算存在局部极小问题，必须通过改进完善。

由于BP网络训练中稳定性要求学习效率很小，所以梯度下降法使得训练很慢。动量法因为学习率的提高通常比单纯的梯度下降法要快一些，但在实际应用中还是速度不够，这两种方法通常只应用于递增训练。

多层神经网络可以应用于线性系统和非线性系统中，对于任意函数模拟逼近。当然，感知器和线性神经网络能够解决这类网络问题。但是，虽然理论上是可行的，但实际上BP网络并不一定总能有解。

对于非线性系统，选择合适的学习率是一个重要的问题。在线性网络中，学习率过大会导致训练过程不稳定。相反，学习率过小又会造成训练时间过长。和线性网络不同，对于非线性多层网络很难选择很好的学习率。对那些快速训练算法，缺省参数值基本上都是最有效的设置。

非线性网络的误差面比线性网络的误差面复杂得多，问题在于多层网络中非线性传递函数有多个局部最优解。寻优的过程与初始点的选择关系很大，初始点如果更靠近局部最优点，而不是全局最优点，就不会得到正确的结果，这也是多层网络无法得到最优解的一个原因。为了解决这个问题，在实际训练过程中，应重复选取多个初始点进行训练，以保证训练结果的全局最优性。

网络隐层神经元的数目也对网络有一定的影响。神经元数目太少会造成网络的不适性，而神经元数目太多又会引起网络的过适性。

### 2 RBF（径向基）神经网络
    径向基函数(RBF-Radial Basis Function)神经网络是由J.Moody和C.Darken在80年代末提出的一种神经网络,它是具有单隐层的三层前馈网络。由于它模拟了人脑中局部调整、相互覆盖接收域（或称感受野-Receptive Field）的神经网络结构，因此，RBF网络是一种局部逼近网络，它能够以任意精度逼近任意连续函数，特别适合于解决分类问题。

 

#### 2.1 主要功能
图像处理，语音识别，时间系列预测，雷达原点定位，医疗诊断，错误处理检测，模式识别等。RBF网络用得最多之处是用于分类，在分类之中，最广的还是模式识别问题，次之是时间序列分析问题。

 

#### 2.2 优点及其局限性
（一） 优点：

神经网络有很强的非线性拟合能力，可映射任意复杂的非线性关系，而且学习规则简单，便于计算机实现。具有很强的鲁棒性、记忆能力、非线性映射能力以及强大的自学习能力，因此有很大的应用市场。

 ①它具有唯一最佳逼近的特性，且无局部极小问题存在。

②    RBF神经网络具有较强的输入和输出映射功能，并且理论证明在前向网     络中RBF网络是完成映射功能的最优网络。

③    网络连接权值与输出呈线性关系。

④    分类能力好。

⑤    学习过程收敛速度快

（二）局限性：

①    最严重的问题是没能力来解释自己的推理过程和推理依据。

②    不能向用户提出必要的询问，而且当数据不充分的时候，神经网络就无法进行工作。

③    把一切问题的特征都变为数字，把一切推理都变为数值计算，其结果势必是丢失信息。

④    理论和学习算法还有待于进一步完善和提高。

⑤    隐层基函数的中心是在输入样本集中选取的, 这在许多情况下难以反映出系统真正的输入输出关系, 并且初始中心点数太多; 另外优选过程会出现数据病态现象。

### 3 感知器神经网络
     是一个具有单层计算神经元的神经网络，网络的传递函数是线性阈值单元。原始的感知器神经网络只有一个神经元。主要用来模拟人脑的感知特征，由于采取阈值单元作为传递函数，所以只能输出两个值，适合简单的模式分类问题。当感知器用于两类模式分类时，相当于在高维样本空间用一个超平面将两类样本分开，但是单层感知器只能处理线性问题，对于非线性或者线性不可分问题无能为力。假设p是输入向量，w是权值矩阵向量，b为阈值向量，由于其传递函数是阈值单元，也就是所谓的硬限幅函数，那么感知器的决策边界就是wp+b，当wp+b>=0时，判定类别1，否则判定为类别2。

#### 3.1 主要功能
主要用于分类。

#### 3.2 优点及其局限性
感知器模型简单易于实现，缺点是仅能解决线性可分问题。解决线性不可分问题途径：一是采用多层感知器模型，二是选择功能更加强大的神经网络模型。

### 4 线性神经网络
线性神经网络是比较简单的一种神经网络，由一个或者多个线性神经元构成。采用线性函数作为传递函数，所以输出可以是任意值。线性神经网络可以采用基于最小二乘LMS的Widrow－Hoff学习规则调节网络的权值和阈值，和感知器一样，线性神经网络只能处理反应输入输出样本向量空间的线性映射关系，也只能处理线性可分问题。目前线性神经网络在函数拟合、信号滤波、预测、控制等方面有广泛的应用。线性神经网络和感知器网络不同，它的传递函数是线性函数，输入和输出之间是简单的纯比例关系，而且神经元的个数可以是多个。只有一个神经元的线性神经网络仅仅在传递函数上和感知器不同，前者是线性函数的传递函数，后者是阈值单元的传递函数，仅此而已。

#### 4.1 主要功能
（1）线性预测；

（2）自适应滤波噪声抵消；

（3）自适应滤波系统辨识；

#### 4.2优点及其局限性
线性神经网络只能反应输入和输出样本向量空间的线性映射关系。由于线性神经网络的误差曲面是一个多维抛物面，所以在学习速率足够小的情况下，对于基于最小二乘梯度下降原理进行训练的神经网络总是可以找到一个最优解。尽管如此，对线性神经网络的训练并不能一定总能达到零误差。线性神经网络的训练性能要受到网络规模、训练集大小的限制。若神经网络的自由度（所有权值和阈值的总个数）小于样本空间中输入－输出向量的个数，而且各样本向量线性无关，则网络不可能达到零误差，只能得到一个使得网络的误差最小的解。反之，如果网络的自由度大于样本集的个数，则会得到无穷多个可以使得网络误差为零的解。

另外对超定系统、不定系统、线性相关向量的情况还有一些其他的限制。

### 5自组织神经网络
在生物神经细胞中存在一种特征敏感细胞，这种细胞只对外界信号刺激的某一特征敏感，并且这种特征是通过自学习形成的。在人脑的脑皮层中，对于外界信号刺激的感知和处理是分区进行的，有学者认为，脑皮层通过邻近神经细胞的相互竞争学习，自适应的发展称为对不同性质的信号敏感的区域。根据这一特征现象，芬兰学者Kohonen提出了自组织特征映射神经网络模型。他认为一个神经网络在接受外界输入模式时，会自适应的对输入信号的特征进行学习，进而自组织成不同的区域，并且在各个区域对输入模式具有不同的响应特征。在输出空间中，这些神经元将形成一张映射图，映射图中功能相同的神经元靠的比较近，功能不同的神经元分的比较开，自组织特征映射网络也是因此得名。

自组织映射过程是通过竞争学习完成的。所谓竞争学习是指同一层神经元之间相互竞争，竞争胜利的神经元修改与其连接的连接权值的过程。竞争学习是一种无监督学习方法，在学习过程中，只需要向网络提供一些学习样本，而无需提供理想的目标输出，网络根据输入样本的特性进行自组织映射，从而对样本进行自动排序和分类。

自组织神经网络包括自组织竞争网络、自组织特征映射网络、学习向量量化等网络结构形式。

#### 5.1 自组织竞争网络
竞争学习网络的结构：假设网络输入为R维，输出为S个，典型的竞争学习网络由隐层和竞争层组成，与径向基函数网络的神经网络模型相比，不同的就是竞争传递函数的输入是输入向量p与神经元权值向量w之间的距离取负以后和阈值向量b的和，即ni=-||wi-p||+bi。网络的输出由竞争层各神经元的输出组成，除了在竞争中获胜的神经元以外，其余的神经元的输出都是0，竞争传递函数输入向量中最大元素对应的神经元是竞争的获胜者，其输出固定是1。

竞争学习网络的训练：竞争学习网络依据Kohonen学习规则和阈值学习规则进行训练，竞争网络每进行一步学习，权值向量与当前输入向量最为接近的神经元将在竞争中获胜，网络依据Kohonen准则对这个神经元的权值进行调整。假设竞争层中第i个神经元获胜，其权值向量Wi将修改为：Wi(k)=Wi(k-1)-alpha*(p(k)-Wi(k-1))。按照这一规则，修改后的神经元权值向量将更加接近当前的输入。经过这样调整以后，当下一此网络输入类似的向量时，这一神经元就很有可能在竞争中获胜，如果输入向量与该神经元的权值向量相差很大，则该神经元极有可能落败。随着训练的进行，网络中的每一个节点将代表一类近似的向量，当接受某一类向量的输入时，对应类别的神经元将在竞争中获胜，从而网络就具备了分类功能。

#### 5.2 自组织特征映射网络
自组织特征映射网络SOFM的构造时基于人类大脑皮质层的模仿。在人脑的脑皮层中，对外界信号刺激的感知和处理是分区进行的，因此自组织特征映射网络不仅仅要对不同的信号产生不同的响应，即与竞争学习网络一样具有分类功能。而且还要实现功能相同的神经元在空间分布上的聚集。因此自组织特征映射网络在训练时除了要对获胜的神经元的权值进行调整之外，还要对获胜神经元邻域内所有的神经元进行权值修正，从而使得相近的神经元具有相同的功能。自组织特征映射网络的结构域竞争学习网络的结构完全相同，只是学习算法有所区别而已。

稳定时，每一邻域的所有节点对某种输入具有类似的输出，并且这聚类的概率分布与输入模式的概率分布相接近。

#### 5.3 学习向量量化网络
学习向量量化网络由一个竞争层和一个线性层组成，竞争层的作用仍然是分类，但是竞争层首先将输入向量划分为比较精细的子类别，然后在线性层将竞争层的分类结果进行合并，从而形成符合用户定义的目标分类模式，因此线性层的神经元个数肯定比竞争层的神经元的个数要少。

学习向量量化网络的训练：学习向量量化网络在建立的时候，竞争层和线性层之间的连接权重矩阵就已经确定了。如果竞争层的某一神经元对应的向量子类别属于线性层的某个神经元所对应的类别，则这两个神经元之间的连接权值等于1，否则两者之间的连接权值为0，这样的权值矩阵就实现了子类别到目标类别的合并。根据这一原则，竞争层和线性层之间的连接权重矩阵的每一列除了一个元素为1之外，其余元素都是0。1在该列中的位置表示了竞争层所确定的子类别属于哪一种目标类别（列中的每一个位置分别表示一种目标类别）。在建立网络时，每一类数据占数据总数的百分比是已知的，这个比例恰恰就是竞争层神经元归并到线性层各个输出时所依据的比例。由于竞争层和线性层之间的连接权重矩阵是事先确定的，所以在网络训练的时候只需要调整竞争层的权值矩阵。

#### 5.4 主要功能
特别适合于解决模式分类和识别方面的应用问题。

#### 5.5 优点及其局限性
SOFM网络（自组织特征映射网络）的最大优点是网络输出层引入了拓扑结构，从而实现了对生物神经网络竞争过程的模拟。

LVQ网络（学习向量量化网路）则在竞争学习的基础山引入了有监督的学习算法，被认为是SOFM算法的扩展形式。

常用的结合方法是，将学习向量算法作为自组织映射算法的补充，在输出层应用具有拓扑结构的自组织映射网络结构，一次采用自组织映射学习算法和学习矢量量化算法对网络进行两次训练。

### 6 反馈神经网络
前面介绍的网络都是前向网络，实际应用中还有另外一种网络——反馈网络。在反馈网络中，信息在前向传递的同时还要进行反向传递，这种信息的反馈可以发生在不同网络层的神经元之间，也可以只局限于某一层神经元上。由于反馈网络属于动态网络，只有满足了稳定条件，网络才能在工作了一段时间之后达到稳定状态。反馈网络的典型代表是Elman网络和Hopfield网络。

#### 6.1 Elman神经网络
Elman网络由若干个隐层和输出层构成，并且在隐层存在反馈环节，隐层神经元采用正切sigmoid型函数作为传递函数，输出层神经元传递函数为纯线性函数，当隐层神经元足够多的时候，Elman网络可以保证网络以任意精度逼近任意非线性函数。

#### 6.2 Hopfield网络
Hopfield网络主要用于联想记忆和优化计算。联想记忆是指当网络输入某一个向量之后，网络经过反馈演化，从网络的输出端得到另外一个向量，这样输出向量称为网络从初始输入向量联想得到的一个稳定的记忆，也就是网络的一个平衡点。优化计算是指某一问题存在多个解法的时候，可以设计一个目标函数，然后寻求满足折椅目标的最优解法。例如在很多情况下可以把能量函数看作是目标函数，得到最优解法需要使得能量函数达到极小值，也就是所谓的能量函数的稳定平衡点。总之，Hopfield网络的设计思想就是在初始输入下，使得网络经过反馈计算，最后达到稳定状态，这时候的输出就是用户需要的平衡点。

#### 6.3 主要应用
Elman网络主要用于信号检测和预测方面，Hopfield网络主要用于联想记忆、聚类以及优化计算等方面。

#### 6.4 优点及其局限性
（一）Hopfield神经网络

对于Hopfield神经网络而言, 都存在以下问题:

(1)在具体神经网络实现中要保证连接权矩阵是对称的;

(2)在实际的神经网络实现中, 总会存在信息的传输延迟, 这些延迟对神经网络的特性有影响。

(3)神经网络实现中的规模问题, 即集成度问题。

（二）Elman神经网络

Elman神经网络模型与其他神经网络模型一样，具有输入层、隐层和输出层，具有学习期和工作期，因此具有自组织、自学习的特征。另外，由于在Elman神经网络模型中增加了隐层及输出层节点的反馈，因而更进一步地增强了网络学习的精确性和容错性。

 

利用Elman神经网络建立的网络模型，对具有非线性时间序列特征的其它应用领域都具有较好地应用前景，它具有较强的鲁棒性、良好的泛化能力、较强的通用性和客观性，充分显示出神经网络方法的优越性和合理性，这种神经网络方法在其它领域预测和评价方面的使用将具有较好的实际应用价值。

 

### 7.其它：

收集了一些资料，还有另一个版本，有重叠：

ANNs指使用第一代或第二代神经元模型的算法


unsupervised learning （聚类）

1、其他聚类：
SOM
autoencoder
2、Deep learning，分三类，方法完全不同，连神经元都不一样
前馈预测：见3
反馈预测：stacked sparse autoencoder（聚类）, predictive coding (属RNN，聚类)
交互预测：Deep belief net (DBN,属RNN，聚类+分类)
3、前馈神经网络（分类）
perceptron
BP
RBF
前馈Deep learning：convolutional neural network (CNN)
4、Recurrent NN类
hopfield
Boltzmann machine 及变种 
echo state network
5、其他工程用算法的神经网络版本，数量太多，简单写几个
强化学习如TD（reinforcement learning）
PCA