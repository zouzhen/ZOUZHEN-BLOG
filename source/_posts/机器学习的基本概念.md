---
title: 机器学习的基本概念
date: 2018-10-29 15:17:56
categories: 机器学习算法
tags: 概念
---

机器学习和深度学习的关系在此不做赘述，本文主要说明机器学习的基本知识

## 学习算法

机器学习算法是一种能够从数据中学习的算法，Mitchell（1997）提供了一个简洁的定义：“对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。”

### **任务T**

通常机器学习任务定义为机器学习系统应该如何处理样本（example）。样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的特征（feature）的集合。  

一些非常常见的机器学习的任务列举如下：

- 分类
- 输入缺失分类
- 回归
- 转录
- 机器翻译
- 结构化输出
- 异常检测
- 合成和采样
- 确实值填补
- 去噪
- 密度估计或概率质量函数估计  

当然，还有很多其他同类型或其他类型的任务，这里我们列举的任务类型只是用来介绍机器学习可以做哪些任务。

### **性能度量P**

为了评估机器学习算法的能力，引入了对其性能的定量度量。通常性能度量P是特定与系统执行的任务T而言的。

这就需要我们针对不同的的任务来寻找相对应的性能度量标准。

### **经验E**

根据学习过程中的不同经验，机器学习算法可以大致分类为无监督（unsupervised）算法和监督（supervised）算法。  

- 无监督学习算法（unsupervised learning algorithm）：训练包含有很多特征的数据集，然后学习出这个数据集上有用的结构性质。
- 监督学习算法（supervised learning algorithm）：训练含有很多特征的数据集，不过数据集中的样本都有一个标签（label）或者目标（target）。

大部分学习算法可以被理解为在整个 **数据集（dataset）** 上获取经验。数据集可以用很多不同方式来表示，在所有的情况下，数据集都是样本的集合，而样本是特征的集合。

- 表示数据集的常用方法是 **设计矩阵（design matrix）**

## 容量、过拟合和欠拟合

机器学习的主要挑战是我们的算法必须能够在先前未观测到的新输入上表现良好，而不只是在训练集上表现良好。在先前未观测到的输入上表现良好的能力被称为 **泛化（generalization）**。

- 训练误差（training error）：使用训练集，进行性能度量的一个量。来优化模型
- 泛化误差（generalization errot）：（也被称为测试误差（test error）），使用测试集，进行性能度量的一个量。

训练集和测试集通过数据集上被称为数据生成过程（data generating process）的概率分部生成。在生成数据集时，不会提前固定参数，然后采样得到两个数据集，一般是先采样得到训练集，然后挑选参数去降低训练集误差，然后采样得到测试集。在这个过程中，测试误差会大于或等于训练误差期望。以下是决定机器学习算法效果是否好的因素：  

- 降低训练误差
- 缩小训练误差和测试误差的差距  

这两个因素对应机器学习的两个主要挑战：**欠拟合（underfitting）和过拟合（overfitting）**。

通过调整模型的容量（capacity）可以控制模型是否偏向于过拟合或者欠拟合。通俗来讲，模型的容量是指其拟合各种函数的能力。容量低的模型可能很难拟合训练集，容量高的模型可能会过拟合，因为记住了不适用于测试集的训练集性质。一种控制训练算法容量的方法是选择假设空间（hypothesis space），即学习算法可以选择为解决方法的函数集。容量不仅取决与模型的选择，模型规定了调整参数降低训练目标时，学习算法可以从哪些函数中选择函数。这被称为模型的表示容量（representational capacity）。在很多情况下，由于额外的限制因素，比如优化算法的不完美，意味着学习算法的有效容量（effective capacity）可能小于模型的表示容量。

### **没有免费午餐定理**
机器学习的没有免费午餐定理（no free lunch theorem）表明（Wolpert，1996），在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率。换言之，在某种意义上，没有一个机器学习算法总是比其他的要好。

### **正则化**

正则化是指修改学习算法，使其降低泛化误差而非训练误差。正则化是机器学习领域领域的中心问题之一，只有优化能够与其重要性想提并论。

## 超参数和验证集

大多数机器学习算法都有超参数，可以设置来控制算法行为。超参数的值不是通过学习算法本身学习出来的（尽管我们可以设置一个嵌套的学习过程，一个学习算法为另一个学习算法学习出最有超参数）  
用于挑选超参数的数据子集被称为验证集

### **交叉验证**

当数据集太小时，也有替代方法允许我们使用所有的样本估计平均测试误差，代价是增加了计算量。最常用的方法是K-折交叉验证过程。

- K-折交叉验证：将数据集分成k个不重合的子集，测试误差可以估计为k次计算后的平均测试误差。在第i次测试时，数据的第i个自己用于测试集，其他的数据用于训练集，但是带来的一个问题是不存在平均误差方差的无偏估计（Bengio and Grandvalet，2004）

## 估计、偏差和方差

统计领域为我们提供了很多工具来实现机器学习目标，不仅可以解决训练集上的任务，还可以泛化。基本的概念，例如参数估计、偏差和方差，对于正式的刻画泛化、欠拟合和过拟合都非常有帮助。

### **点估计**

点估计试图为一些感兴趣的量提供单个“最优”预测。一般地，感兴趣的量可以是单个参数，或是某些参数模型中的一个向量参数。

- 函数估计：有时我们会关注函数估计（或函数近似）。

### **偏差**

估计的偏差有 **无偏（unbiased）和渐近无偏（asymptotically unbiased）**。

- 伯努利分布
- 均值的高斯分布估计
- 高斯分布方差估计

    - 样本方差

- 无偏样本方差

### **方差和标准差**

我们有时会考虑估计量的另一个性质是它作为数据样本的函数，期望的变化程度是多少。正如我们可以计算估计量的期望来决定它的偏差，我们也可以计算它的方差