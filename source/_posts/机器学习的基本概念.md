---
title: 机器学习的基本概念
date: 2018-10-29 15:17:56
categories: 机器学习算法
tags: 概念
---

机器学习和深度学习的关系在此不做赘述，本文主要说明机器学习的基本知识

## 学习算法

机器学习算法是一种能够从数据中学习的算法，Mitchell（1997）提供了一个简洁的定义：“对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。”

### **任务T**

通常机器学习任务定义为机器学习系统应该如何处理样本（example）。样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的特征（feature）的集合。  

一些非常常见的机器学习的任务列举如下：

- 分类
- 输入缺失分类
- 回归
- 转录
- 机器翻译
- 结构化输出
- 异常检测
- 合成和采样
- 确实值填补
- 去噪
- 密度估计或概率质量函数估计  

当然，还有很多其他同类型或其他类型的任务，这里我们列举的任务类型只是用来介绍机器学习可以做哪些任务。

### **性能度量P**

为了评估机器学习算法的能力，引入了对其性能的定量度量。通常性能度量P是特定与系统执行的任务T而言的。

这就需要我们针对不同的的任务来寻找相对应的性能度量标准。

### **经验E**

根据学习过程中的不同经验，机器学习算法可以大致分类为无监督（unsupervised）算法和监督（supervised）算法。  

- 无监督学习算法（unsupervised learning algorithm）：训练包含有很多特征的数据集，然后学习出这个数据集上有用的结构性质。
- 监督学习算法（supervised learning algorithm）：训练含有很多特征的数据集，不过数据集中的样本都有一个标签（label）或者目标（target）。

大部分学习算法可以被理解为在整个 **数据集（dataset）** 上获取经验。数据集可以用很多不同方式来表示，在所有的情况下，数据集都是样本的集合，而样本是特征的集合。

- 表示数据集的常用方法是 **设计矩阵（design matrix）**

## 容量、过拟合和欠拟合

机器学习的主要挑战是我们的算法必须能够在先前未观测到的新输入上表现良好，而不只是在训练集上表现良好。在先前未观测到的输入上表现良好的能力被称为 **泛化（generalization）**。

- 训练误差（training error）：使用训练集，进行性能度量的一个量。来优化模型
- 泛化误差（generalization errot）：（也被称为测试误差（test error）），使用测试集，进行性能度量的一个量。

训练集和测试集通过数据集上被称为数据生成过程（data generating process）的概率分部生成。在生成数据集时，不会提前固定参数，然后采样得到两个数据集，一般是先采样得到训练集，然后挑选参数去降低训练集误差，然后采样得到测试集。在这个过程中，测试误差会大于或等于训练误差期望。以下是决定机器学习算法效果是否好的因素：  

- 降低训练误差
- 缩小训练误差和测试误差的差距  

这两个因素对应机器学习的两个主要挑战：**欠拟合（underfitting）和过拟合（overfitting）**。

通过调整模型的容量（capacity）可以控制模型是否偏向于过拟合或者欠拟合。通俗来讲，模型的容量是指其拟合各种函数的能力。容量低的模型可能很难拟合训练集，容量高的模型可能会过拟合，因为记住了不适用于测试集的训练集性质。一种控制训练算法容量的方法是选择假设空间（hypothesis space），即学习算法可以选择为解决方法的函数集。容量不仅取决与模型的选择，模型规定了调整参数降低训练目标时，学习算法可以从哪些函数中选择函数。这被称为模型的表示容量（representational capacity）。在很多情况下，由于额外的限制因素，比如优化算法的不完美，意味着学习算法的有效容量（effective capacity）可能小于模型的表示容量。

### **没有免费午餐定理**
机器学习的没有免费午餐定理（no free lunch theorem）表明（Wolpert，1996），在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率。换言之，在某种意义上，没有一个机器学习算法总是比其他的要好。

### **正则化**

正则化是指修改学习算法，使其降低泛化误差而非训练误差。正则化是机器学习领域领域的中心问题之一，只有优化能够与其重要性想提并论。

## 超参数和验证集

大多数机器学习算法都有超参数，可以设置来控制算法行为。超参数的值不是通过学习算法本身学习出来的（尽管我们可以设置一个嵌套的学习过程，一个学习算法为另一个学习算法学习出最有超参数）  
用于挑选超参数的数据子集被称为验证集

### **交叉验证**

当数据集太小时，也有替代方法允许我们使用所有的样本估计平均测试误差，代价是增加了计算量。最常用的方法是K-折交叉验证过程。

- K-折交叉验证：将数据集分成k个不重合的子集，测试误差可以估计为k次计算后的平均测试误差。在第i次测试时，数据的第i个自己用于测试集，其他的数据用于训练集，但是带来的一个问题是不存在平均误差方差的无偏估计（Bengio and Grandvalet，2004）

## 估计、偏差和方差

统计领域为我们提供了很多工具来实现机器学习目标，不仅可以解决训练集上的任务，还可以泛化。基本的概念，例如参数估计、偏差和方差，对于正式的刻画泛化、欠拟合和过拟合都非常有帮助。

### **点估计**

点估计试图为一些感兴趣的量提供单个“最优”预测。一般地，感兴趣的量可以是单个参数，或是某些参数模型中的一个向量参数。

- 函数估计：有时我们会关注函数估计（或函数近似）。

### **偏差**

估计的偏差有 **无偏（unbiased）和渐近无偏（asymptotically unbiased）**。

- 伯努利分布
- 均值的高斯分布估计
- 高斯分布方差估计

    - 样本方差

- 无偏样本方差

### **方差和标准差**

我们有时会考虑估计量的另一个性质是它作为数据样本的函数，期望的变化程度是多少。正如我们可以计算估计量的期望来决定它的偏差，我们也可以计算它的方差。估计量的方差就是一个方差，另外，方差的平方根被称为标准差（standard error）。

### **权衡偏差和方差以最小化均方误差**

偏差和方差度量着估计量的两个不同误差来源。偏差度量着偏离真实函数或参数的误差期望，而方差度量这数据上任意特定采样可能导致的估计期望的偏差。  
当面对一个偏差更大的估计和一个方差更大的估计时，我们如何进行选择。判断这种权衡最常用的方法是**交叉验证**，我们还可以比较这些估计的**均方误差（mean squared error， MSE）**。MSE度量着估计和真实值之间平方误差的总体期望偏差。

### **一致性**

一致性保证了估计量的偏差会随着样本数目的增多而减少。然而，反过来是不正确的——渐进无偏并不意味着一致性。

## 最大似然估计

之前，我们已经看到过常用估计的定义，并分析了它们的性质。但是这些估计是从哪里来的呢？我们希望有些准则可以让我们从不同模型中得到特定函数作为好的估计，而不是猜测某些函数可能是好的估计，然后分析其偏差和方差。  
最常用的准则是最大似然估计。

### **条件对数似然和均方误差**

- 线性回归作为最大似然

### **最大似然的性质**

最大似然估计最吸引人的地方在于，它被证明当样本数目趋向于无穷大时，就收敛率而言是最好的渐进估计。  
在合适的条件下，最大似然估计具有一致性，意味着训练样本数目趋向于无穷大时，参数的最大似然估计会收敛到参数的真实值。

## 贝叶斯统计

前面讨论的属于**频率派统计（frequentist statistics）** 方法和基于估计单一值的方法，然后基于该估计作所有的预测。另一种方法是在做预测时会考虑所有可能的值。后者属于**贝叶斯统计（Bayesian statistics）**的范畴。  

- 贝叶斯线性回归

### **最大后验（MAP）估计**

 MAP贝叶斯推断提供了一个直观的方法来设计复杂但可解释的正则化项。例如，更复杂的惩罚项可以通过混合高斯分部作为先验得到，而不是一个单独的高斯分布（Nowlan and Hinton，1992）

## 监督学习算法

粗略的说，监督学习算法是给定一组输入x和输出y的训练集，学习如何关联输入和输出。在许多情况下，输出y很难自动收集，必须由人来提供“监督”，不过该术语仍然适用于训练集目标可以被自动收集的情况。  

### **概率监督学习**

### **支持向量机**

支持向量机（support vector machine，SVM）是监督学习中最有影响力的方法之一。类似于逻辑回归，但支持向量机不输出概率，只输出类别。  
支持向量机的一个重要创新是核技巧（kernal trick）。核技巧观察到许多机器学习算法都可以写成样本间点积的形式。  
最常用的核函数是高斯核（Gaussian kernel），这个核也被称为径向基函数（radial basis function，RBF）核。  
支持向量机不是唯一可以使用核技巧来增强的算法，许多其他的线性模型也可以通过这种方式来增强。使用核技巧的算法类别被称为和机器（kernel machine）或核方法（kernel method）。核机器的一个主要缺点是计算决策函数的成本关于训练样本的数目是线性的。而训练的样本则被称做支持向量（support vector）。

### **其他简单的机器学习算法**

- 决策树

## 无监督学习算法

无监督算法只处理“特征”，不操作监督信号。监督和无监督算法之间的区别没有规范严格的定义，因为没有客观的判断来区分监督者提供的值是特征还是目标。通俗的说，无监督学习的大多数尝试是指从不需要人为注释的样本的分布中抽取信息。该术语通常与密度估计相关，学习从分布中采样、学习从分布中去噪、寻找数据分布的流形或是将数据中相关的样本聚类。

### **主成分分析**

### **k-均值聚类**

## 随机梯度下降

几乎所有的深度学习算法都用到了一个非常重要的算法：梯度下降算法（stochastic gradient,SGD）。  
机器学习中反复出现的一个问题是好的泛化需要大的训练集，但大的计算集的计算代价也更大。机器学习算法中的代价函数通常可以分解成每个样本的代价函数的总和。  
随机梯度下降的核心是，梯度是期望。期望可使用小规模的样本近似估计。

## 构建机器学习算法

## 构建机器学习算法

## 促使深度学习发展的挑战

- 维度灾难
- 局部不变性和平滑正则化
- 流形学习